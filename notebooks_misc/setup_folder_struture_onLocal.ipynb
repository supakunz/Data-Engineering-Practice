{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a47b58f7",
   "metadata": {},
   "source": [
    "#### ‚úÖ Step-by-Step: ‡πÄ‡∏ï‡∏£‡∏µ‡∏¢‡∏° Local ETL Project ‡πÅ‡∏ö‡∏ö‡∏á‡∏≤‡∏ô‡∏à‡∏£‡∏¥‡∏á"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a1ba8f",
   "metadata": {},
   "source": [
    "##### 1. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67cc88",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "  mkdir etl-sales-pipeline\n",
    "  cd etl-sales-pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a903af89",
   "metadata": {},
   "source": [
    "##### 2. ‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÑ‡∏ü‡∏•‡πå‡∏ó‡∏µ‡πà‡πÅ‡∏ô‡∏∞‡∏ô‡∏≥ (‡∏ï‡∏≤‡∏° best practice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f92ec5",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "etl-sales-pipeline/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ data/                  ‚Üê ‡πÄ‡∏Å‡πá‡∏ö raw, processed, output data\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ raw/\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ processed/\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ output/\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ etl/                   ‚Üê ‡πÇ‡∏Ñ‡πâ‡∏î‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á ETL\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ extract.py         ‚Üê ‡πÇ‡∏´‡∏•‡∏î‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡πÅ‡∏´‡∏•‡πà‡∏á‡∏ï‡πà‡∏≤‡∏á‡πÜ (API, CSV)\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ transform.py       ‚Üê ‡∏à‡∏±‡∏î‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö, clean, enrich ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•\n",
    "‚îÇ   ‚îú‚îÄ‚îÄ load.py            ‚Üê ‡πÄ‡∏ã‡∏ü‡∏•‡∏á‡πÑ‡∏ü‡∏•‡πå / DB / Cloud storage\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ utils.py           ‚Üê ‡∏ü‡∏±‡∏á‡∏Å‡πå‡∏ä‡∏±‡∏ô‡∏ä‡πà‡∏ß‡∏¢ (‡πÄ‡∏ä‡πà‡∏ô logging, date parsing)\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ config/                ‚Üê ‡∏û‡∏ß‡∏Å config file, credential, parameter\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ config.yaml\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ jobs/                  ‚Üê script ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏£‡∏±‡∏ô pipeline ‡∏ï‡∏≤‡∏°‡∏ß‡∏±‡∏ô\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ run_etl.py\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ notebooks/             ‚Üê ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏ó‡∏î‡∏•‡∏≠‡∏á‡∏ö‡∏ô notebook ‡∏ä‡∏±‡πà‡∏ß‡∏Ñ‡∏£‡∏≤‡∏ß\n",
    "‚îÇ   ‚îî‚îÄ‚îÄ eda_transform.ipynb\n",
    "‚îÇ\n",
    "‚îú‚îÄ‚îÄ requirements.txt       ‚Üê ‡∏£‡∏≤‡∏¢‡∏Å‡∏≤‡∏£ dependencies\n",
    "‚îú‚îÄ‚îÄ .env                   ‚Üê ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏•‡∏±‡∏ö (‡πÄ‡∏ä‡πà‡∏ô API key)\n",
    "‚îî‚îÄ‚îÄ README.md\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b918c5f3",
   "metadata": {},
   "source": [
    "| ‡πÇ‡∏ü‡∏•‡πÄ‡∏î‡∏≠‡∏£‡πå     | ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö                                                                                 | ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á                                          |\n",
    "| ------------ | ----------------------------------------------------------------------------------------- | ------------------------------------------------- |\n",
    "| `raw/`       | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡πâ‡∏ô‡∏â‡∏ö‡∏±‡∏ö‡∏ó‡∏µ‡πà‡∏¢‡∏±‡∏á‡πÑ‡∏°‡πà‡πÅ‡∏ï‡∏∞‡∏ï‡πâ‡∏≠‡∏á                                                             | raw CSV, JSON ‡∏à‡∏≤‡∏Å API                             |\n",
    "| `processed/` | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£ clean / transform ‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô ‡πÄ‡∏ä‡πà‡∏ô remove null, filter outliers, cast type | `book_sales_cleaned.csv`                          |\n",
    "| `output/`    | ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•, BI, dashboard, export to report                                | `monthly_sales_summary.csv`, `top_books_2024.csv` |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4237b433",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "source": [
    "##### 3. ‡∏ï‡∏¥‡∏î‡∏ï‡∏±‡πâ‡∏á environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6855b50b",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "python -m venv venv\n",
    "source venv/bin/activate      # ‡∏ö‡∏ô Linux/macOS\n",
    "# ‡∏´‡∏£‡∏∑‡∏≠ venv\\Scripts\\activate  # ‡∏ö‡∏ô Windows\n",
    "\n",
    "pip install pyspark python-dotenv pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dee40a",
   "metadata": {
    "vscode": {
     "languageId": "ini"
    }
   },
   "outputs": [],
   "source": [
    "# requirements.txt\n",
    "pyspark\n",
    "python-dotenv\n",
    "pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcc27d9c",
   "metadata": {},
   "source": [
    "##### 4. ‡∏™‡∏£‡πâ‡∏≤‡∏á `config.yaml` ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f177eb",
   "metadata": {
    "vscode": {
     "languageId": "latex"
    }
   },
   "outputs": [],
   "source": [
    "data_source:\n",
    "  csv_path: \"data/raw/sales_data.csv\"\n",
    "\n",
    "output:\n",
    "  parquet_path: \"data/output/sales_summary.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de1294",
   "metadata": {},
   "source": [
    "##### 5. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á `extract.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a00ab24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def extract_csv(spark: SparkSession, path: str):\n",
    "    return spark.read.option(\"header\", True).csv(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a9e40be",
   "metadata": {},
   "source": [
    "##### 6. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á `transform.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2fc935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.functions import col, to_date\n",
    "\n",
    "def clean_sales_data(df: DataFrame) -> DataFrame:\n",
    "    return (\n",
    "        df.withColumn(\"amount\", col(\"amount\").cast(\"float\"))\n",
    "          .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
    "          .dropna()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a59081f",
   "metadata": {},
   "source": [
    "##### 7. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á `load.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16620a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_parquet(df, path: str):\n",
    "    df.write.mode(\"overwrite\").parquet(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f9ff9e",
   "metadata": {},
   "source": [
    "##### 8. Script ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏£‡∏±‡∏ô‡∏á‡∏≤‡∏ô `jobs/run_etl.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41567164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import yaml\n",
    "from etl.extract import extract_csv # # type: ignore\n",
    "from etl.transform import clean_sales_data # # type: ignore\n",
    "from etl.load import save_to_parquet # type: ignore\n",
    "\n",
    "# Load config\n",
    "with open(\"config/config.yaml\", \"r\") as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SalesETL\").getOrCreate()\n",
    "\n",
    "# ETL flow\n",
    "df_raw = extract_csv(spark, config[\"data_source\"][\"csv_path\"])\n",
    "df_clean = clean_sales_data(df_raw)\n",
    "save_to_parquet(df_clean, config[\"output\"][\"parquet_path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c730722",
   "metadata": {},
   "source": [
    "##### 9. ‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏ô pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73874b2f",
   "metadata": {
    "vscode": {
     "languageId": "bat"
    }
   },
   "outputs": [],
   "source": [
    "python jobs/run_etl.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3512ac7",
   "metadata": {},
   "source": [
    "##### 10.‡∏´‡∏≤‡∏Å‡∏≠‡∏¢‡∏≤‡∏Å‡∏î‡∏π‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå Parquet ‡πÅ‡∏ö‡∏ö‡πÄ‡∏£‡πá‡∏ß ‡πÜ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f69407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet(\"data/output/sales_summary.parquet\") # type: ignore\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7758b2f",
   "metadata": {},
   "source": [
    "##### üîÅ ‡∏™‡∏£‡∏∏‡∏õ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd3c44d",
   "metadata": {},
   "source": [
    "| Step | Task                                         |\n",
    "| ---- | -------------------------------------------- |\n",
    "| 1.   | ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏Ñ‡∏£‡∏á‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Å‡∏ï‡πå                       |\n",
    "| 2.   | ‡πÅ‡∏¢‡∏Å‡πÑ‡∏ü‡∏•‡πå extract, transform, load             |\n",
    "| 3.   | ‡πÉ‡∏ä‡πâ config / .env ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏¢‡∏Å parameter         |\n",
    "| 4.   | ‡∏£‡∏±‡∏ô‡∏ú‡πà‡∏≤‡∏ô script (`jobs/run_etl.py`)           |\n",
    "| 5.   | ‡∏ó‡∏î‡∏™‡∏≠‡∏ö‡∏Å‡∏±‡∏ö sample data ‡∏à‡∏£‡∏¥‡∏á (CSV / JSON / API) |\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
